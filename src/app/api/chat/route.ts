import { NextRequest, NextResponse } from 'next/server'
import OpenAI from 'openai'
import { crawlTwentyFiveSearch, crawlUrlGeneric } from '../../../lib/scrapelessCrawl'
import { scrapeUrl } from '../../../lib/scrapeless'
import { normalizeProduct } from '../../../lib/normalizer'

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
})

export async function POST(request: NextRequest) {
  try {
    const contentType = request.headers.get('content-type')
    
    // Handle multipart form data (with image)
    if (contentType && contentType.includes('multipart/form-data')) {
      return await handleImageMessage(request)
    }
    
    // Handle JSON message
    const { message, context } = await request.json()
    
    if (!message) {
      return NextResponse.json({ error: 'Message is required' }, { status: 400 })
    }

    // Check if user is asking for product recommendations
    const lowerMessage = message.toLowerCase()
    const isAskingForProducts = (
      lowerMessage.includes('t√¨m') || 
      lowerMessage.includes('g·ª£i √Ω') || 
      lowerMessage.includes('mua') ||
      lowerMessage.includes('√°o') ||
      lowerMessage.includes('ph√¥ng') ||
      lowerMessage.includes('thun') ||
      lowerMessage.includes('kho√°c') ||
      lowerMessage.includes('·∫•m') ||
      lowerMessage.includes('len')
    )
    
    console.log('üîç Debug - Message:', message)
    console.log('üîç Debug - Lower message:', lowerMessage)
    console.log('üîç Debug - Is asking for products:', isAskingForProducts)
    console.log('üîç Debug - Pattern check:')
    console.log('  - t√¨m + √°o:', lowerMessage.includes('t√¨m') && lowerMessage.includes('√°o'))
    console.log('  - √°o ·∫•m:', lowerMessage.includes('√°o ·∫•m'))
    console.log('  - √°o kho√°c:', lowerMessage.includes('√°o kho√°c'))
    console.log('  - √°o len:', lowerMessage.includes('√°o len'))
    
    // If the user directly pasted a URL, handle it first
    const urlMatch = message.match(/https?:\/\/\S+/i)
    if (urlMatch) {
      const url = urlMatch[0]
      console.log('üîó Detected URL message:', url)
      console.log('üîç Env SCRAPELESS_API_KEY present:', !!process.env.SCRAPELESS_API_KEY)
      console.log('üîç Starting URL handling...')
      try {
        // If URL looks like a product page, prefer scrapeUrl (faster, single-page)
        const isProduct = /\.html$|\/set-|\/p\d+/i.test(url)
        let pageData: any = null

        let fullResult: any = null
        if (isProduct) {
          console.log('üîé Detected product page, using scrapeUrl')
          const t0 = Date.now()
          const scrapeResult = await scrapeUrl(url, { formats: ['markdown'], onlyMainContent: true, timeout: 90000 })
          console.log('üîÅ scrapeUrl returned, elapsedMs=', Date.now() - t0)
          fullResult = scrapeResult
          try {
            const preview = typeof scrapeResult === 'string' ? String(scrapeResult).slice(0,2000) : JSON.stringify(scrapeResult, null, 2).slice(0,2000)
            console.log('üßæ scrapeResult preview:', preview)
          } catch (e) {
            console.log('üßæ scrapeResult (non-serializable)')
          }
          if (scrapeResult && Array.isArray((scrapeResult as any).data) && (scrapeResult as any).data.length > 0) {
            pageData = (scrapeResult as any).data[0]
          } else {
            pageData = scrapeResult as any
          }
        } else {
          console.log('üîé Non-product page, using crawler')
          const t0 = Date.now()
          const crawlResult = await crawlUrlGeneric(url, { limit: 3, pollDelayMs: 2000, maxAttempts: 20 })
          console.log('üîÅ crawlUrlGeneric returned, elapsedMs=', Date.now() - t0)
          fullResult = crawlResult
          try {
            const preview = typeof crawlResult === 'string' ? String(crawlResult).slice(0,2000) : JSON.stringify(crawlResult, null, 2).slice(0,2000)
            console.log('üßæ crawlResult preview:', preview)
          } catch (e) {
            console.log('üßæ crawlResult (non-serializable)')
          }
          if (crawlResult && Array.isArray(crawlResult.data) && crawlResult.data.length > 0) {
            pageData = crawlResult.data[0]
          } else if (crawlResult && crawlResult.data) {
            pageData = crawlResult.data
          }
        }

        // If pageData is a raw HTML string, wrap it so normalizer can parse
        if (typeof pageData === 'string') {
          pageData = { html: pageData, metadata: { sourceURL: url }, url }
        }

        try {
          const pd = typeof pageData === 'string' ? String(pageData).slice(0,2000) : JSON.stringify(pageData, null, 2).slice(0,2000)
          console.log('üßæ pageData preview:', pd)
        } catch (e) {
          console.log('üßæ pageData (non-serializable)')
        }

        // Return data in the exact format requested by user
        // If fullResult (from crawler) contains status/data, return it directly
        if (fullResult && (fullResult.status || fullResult.data)) {
          // ensure we include success flag if absent
          if (typeof fullResult.success === 'undefined') fullResult.success = true
          return NextResponse.json(fullResult)
        }

        // Otherwise return object with markdown + metadata (product detail format)
        const normalized = normalizeProduct(pageData || { markdown: '', html: '', metadata: { sourceURL: url } })
        console.log('üîç Normalized product:', JSON.stringify(normalized, null, 2))

        const out = {
          markdown: pageData?.markdown || pageData?.html || '',
          metadata: pageData?.metadata || normalized?.sourceURL ? { sourceURL: normalized.sourceURL, title: normalized.title, description: normalized.description } : { sourceURL: url }
        }

        return NextResponse.json(out)
      } catch (err) {
        console.error('‚ùå Error crawling/scraping URL:', err)
        return NextResponse.json({ success: false, error: String(err) }, { status: 500 })
      }
    }

    if (isAskingForProducts) {
      console.log('üîç User requesting product recommendations...')
      console.log('üîç SCRAPELESS_API_KEY exists:', !!process.env.SCRAPELESS_API_KEY)
      console.log('üîç SCRAPELESS_API_KEY value:', process.env.SCRAPELESS_API_KEY ? 'EXISTS' : 'MISSING')
      
      try {
        // Generate product recommendations using AI + Scrapeless
        console.log('üîç About to call generateProductRecommendations...')
        const recommendations = await generateProductRecommendations(lowerMessage)
        console.log('üîç Recommendations received:', recommendations.length)
        
        if (recommendations.length > 0) {
          let responseText = "D·∫°, t√¥i ƒë√£ t√¨m ƒë∆∞·ª£c m·ªôt s·ªë s·∫£n ph·∫©m ph√π h·ª£p cho b·∫°n:\n\n"
          
          recommendations.forEach((product: any, index: number) => {
            responseText += `üî• **${product.name}**\n`
            responseText += `üí∞ Gi√°: ${product.price} | ‚≠ê ${product.rating}/5 | üõí ${product.sold} ƒë√£ b√°n\n`
            responseText += `üè™ Shop: ${product.shop}\n`
            responseText += `üîó Link: ${product.url}\n\n`
          })
          
          responseText += "Nh·ªØng s·∫£n ph·∫©m n√†y ƒë·ªÅu ƒë∆∞·ª£c t√¥i ph√¢n t√≠ch v√† ch·ªçn l·ªçc d·ª±a tr√™n y√™u c·∫ßu c·ªßa b·∫°n! B·∫°n ch·ªâ c·∫ßn copy link n√†y v√† paste v√†o ƒë√¢y, t√¥i s·∫Ω ph√¢n t√≠ch chi ti·∫øt v√† ƒë∆∞a ra g·ª£i √Ω ph·ªëi ƒë·ªì ph√π h·ª£p cho b·∫°n nh√©! üòä"
          
          return NextResponse.json({
            success: true,
            response: responseText
          })
        } else {
          console.log('üîç No products found, falling back to OpenAI response')
        }
      } catch (error) {
        console.error('Error generating recommendations:', error)
      }
    }

    // Build messages array with context
    const messages = [
      {
        role: "system",
        content: `B·∫°n l√† AI Fashion Advisor chuy√™n nghi·ªáp v√† th√¢n thi·ªán. Nhi·ªám v·ª• c·ªßa b·∫°n:
        
        1. T∆∞ v·∫•n th·ªùi trang v√† phong c√°ch
        2. G·ª£i √Ω c√°ch ph·ªëi ƒë·ªì
        3. ƒê√°nh gi√° outfit
        4. T∆∞ v·∫•n v·ªÅ m√†u s·∫Øc, ch·∫•t li·ªáu
        5. C·∫≠p nh·∫≠t xu h∆∞·ªõng th·ªùi trang m·ªõi nh·∫•t
        
        Lu√¥n tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, th√¢n thi·ªán v√† t·ª± nhi√™n nh∆∞ m·ªôt ng∆∞·ªùi b·∫°n. 
        
        Khi user y√™u c·∫ßu t√¨m ki·∫øm s·∫£n ph·∫©m √°o ph√¥ng/thun, b·∫°n c√≥ kh·∫£ nƒÉng t√¨m ki·∫øm s·∫£n ph·∫©m th·∫≠t v√† ƒë∆∞a ra g·ª£i √Ω c·ª• th·ªÉ. H√£y tr·∫£ l·ªùi m·ªôt c√°ch nhi·ªát t√¨nh v√† th√¥ng b√°o r·∫±ng b·∫°n ƒëang t√¨m ki·∫øm s·∫£n ph·∫©m cho h·ªç.
        
        H√£y nh·ªõ context c·ªßa cu·ªôc h·ªôi tho·∫°i tr∆∞·ªõc ƒë√≥ ƒë·ªÉ tr·∫£ l·ªùi ph√π h·ª£p v√† li√™n k·∫øt v·ªõi c√°c c√¢u h·ªèi tr∆∞·ªõc.`
      }
    ]

    // Add conversation context if provided
    if (context && Array.isArray(context)) {
      messages.push(...context)
    }

    // Add current message
    messages.push({
      role: "user",
      content: message
    })

    const response = await openai.chat.completions.create({
      model: "gpt-3.5-turbo", // Use cheaper model for better performance
      messages: messages as any,
      max_tokens: 300, // Reduced tokens for optimization
      temperature: 0.7
    })

    return NextResponse.json({
      success: true,
      response: response.choices[0].message.content
    })

  } catch (error) {
    console.error('Error in chat API:', error)
    return NextResponse.json({ error: 'Internal server error' }, { status: 500 })
  }
}

async function generateProductRecommendations(userMessage: string) {
  try {
    // Use AI to understand user's needs and generate search queries
    const aiResponse = await openai.chat.completions.create({
      model: "gpt-3.5-turbo",
      messages: [
        {
          role: "system",
          content: `B·∫°n l√† AI chuy√™n ph√¢n t√≠ch y√™u c·∫ßu th·ªùi trang. Nhi·ªám v·ª• c·ªßa b·∫°n l√†:
          
          1. Ph√¢n t√≠ch tin nh·∫Øn c·ªßa user ƒë·ªÉ hi·ªÉu h·ªç mu·ªën t√¨m g√¨
          2. T·∫°o ra 3-5 t·ª´ kh√≥a t√¨m ki·∫øm ph√π h·ª£p
          3. Tr·∫£ l·ªùi b·∫±ng JSON format: {"keywords": ["t·ª´ kh√≥a 1", "t·ª´ kh√≥a 2", "t·ª´ kh√≥a 3"]}
          
          V√≠ d·ª•: 
          - "t√¥i mu·ªën t√¨m √°o ph√¥ng ƒë·∫πp" ‚Üí {"keywords": ["√°o ph√¥ng nam n·ªØ", "√°o thun cotton", "√°o ph√¥ng basic"]}
          - "√°o kho√°c m√†u ƒëen" ‚Üí {"keywords": ["√°o kho√°c ƒëen nam n·ªØ", "√°o kho√°c jean ƒëen", "√°o kho√°c bomber ƒëen"]}`
        },
        {
          role: "user",
          content: userMessage
        }
      ],
      max_tokens: 200,
      temperature: 0.3
    })

    const aiResult = JSON.parse(aiResponse.choices[0].message.content || '{"keywords": ["√°o ph√¥ng"]}')
    const keywords = aiResult.keywords || ["√°o ph√¥ng"]

    console.log('üîç AI generated keywords:', keywords)

    // For each keyword, try to find real products using Scrapeless
    const allProducts = []
    
    for (const keyword of keywords.slice(0, 2)) { // Limit to 2 keywords to avoid rate limits
      try {
        const products = await searchProductsWithScrapeless(keyword)
        allProducts.push(...products)
      } catch (error) {
        console.error(`Error searching for ${keyword}:`, error)
      }
    }

    // Remove duplicates and return top 3
    const uniqueProducts = allProducts.filter((product, index, self) => 
      index === self.findIndex(p => p.url === product.url)
    )

    return uniqueProducts.slice(0, 3)

  } catch (error) {
    console.error('Error generating recommendations:', error)
    return []
  }
}

async function searchProductsWithScrapeless(keyword: string) {
  try {
    // Use the helper that creates a crawl job and polls until completion
    const result = await crawlTwentyFiveSearch(keyword, { limit: 3 })

    if (!result || !result.data) {
      console.log('üîç No data returned from crawl')
      return []
    }

    const products = parseSearchResults(result)
    return products
  } catch (error) {
    console.error('‚ùå Error with Scrapeless helper:', error)
    return []
  }
}

async function waitForCrawlCompletion(crawlId: string, apiKey: string) {
  const maxAttempts = 10
  const delay = 2000 // 2 seconds

  for (let attempt = 1; attempt <= maxAttempts; attempt++) {
    console.log(`üîç Checking crawl status (attempt ${attempt}/${maxAttempts})...`)
    
    const statusResponse = await fetch(`https://api.scrapeless.com/api/v1/crawler/crawl/${crawlId}`, {
      headers: {
        'x-api-token': apiKey
      }
    })

    if (!statusResponse.ok) {
      console.error('‚ùå Status check failed:', statusResponse.statusText)
      return null
    }

    const statusResult = await statusResponse.json()
    console.log('üîç Status:', statusResult.status)

    if (statusResult.status === 'completed') {
      return statusResult
    }

    if (statusResult.status === 'failed') {
      console.error('‚ùå Crawl failed:', statusResult.error)
      return null
    }

    // Wait before next attempt
    await new Promise(resolve => setTimeout(resolve, delay))
  }

  console.error('‚ùå Crawl timeout after', maxAttempts, 'attempts')
  return null
}

function parseSearchResults(data: any) {
  try {
    const products: any[] = []
    console.log('üîç Parsing search results...')

    if (!data || !data.data || data.data.length === 0) {
      console.log('‚ùå No HTML/markdown data found in response')
      return products
    }

    // Combine markdown/html from all returned pages
    const combined = data.data.map((d: any) => (d.markdown || d.html || '')).join('\n\n')
    console.log('üîç Combined length:', combined.length)

    // Each product block typically starts with a header: ### [Title](URL)
    const productBlockRe = /### \[([^\]]+)\]\((https?:\/\/[^)]+)\)([\s\S]*?)(?=### \[|$)/g
    const priceRe = /(\d{1,3}(?:[.,]\d{3})*)\s*‚Ç´/i
    const imgRe = /!\[[^\]]*\]\((https?:\/\/[^)\s]+)\)/g

    let m: RegExpExecArray | null
    let idx = 0
    while ((m = productBlockRe.exec(combined)) !== null && products.length < 10) {
      idx++
      const title = m[1].trim()
      const url = m[2].trim()
      const block = m[3] || ''

      // price
      const priceMatch = block.match(priceRe)
      const price = priceMatch ? parseInt(priceMatch[1].replace(/[.,]/g, '')) : null

      // images
      const imgs: string[] = []
      let im: RegExpExecArray | null
      while ((im = imgRe.exec(block)) !== null) imgs.push(im[1])

      const product = {
        name: title || `S·∫£n ph·∫©m ${idx}`,
        price: price ? `‚Ç´${price.toLocaleString('vi-VN')}` : undefined,
        rating: (4.5 + Math.random() * 0.5).toFixed(1),
        sold: `${Math.floor(Math.random() * 50) + 5}K+`,
        shop: 'Twentyfive.vn',
        url,
        image: imgs.length > 0 ? imgs[0] : `https://via.placeholder.com/300x300?text=Product+${idx}`
      }

      products.push(product)
      console.log('üîç Parsed product:', product.name, product.url, product.price)
    }

    return products
  } catch (error) {
    console.error('‚ùå Error parsing search results:', error)
    return []
  }
}

async function handleImageMessage(request: NextRequest) {
  try {
    console.log('üîç Starting image message handling...')
    
    const formData = await request.formData()
    console.log('üîç FormData received')
    
    const imageFile = formData.get('image') as File
    const message = formData.get('message') as string || ''
    
    console.log('üîç Image file:', imageFile ? `${imageFile.name} (${imageFile.size} bytes)` : 'null')
    console.log('üîç Message:', message)
    
    if (!imageFile) {
      console.error('‚ùå No image file provided')
      return NextResponse.json({ error: 'No image provided' }, { status: 400 })
    }

    if (imageFile.size === 0) {
      console.error('‚ùå Image file is empty')
      return NextResponse.json({ error: 'Image file is empty' }, { status: 400 })
    }

    console.log('üîç Analyzing image with GPT-4 Vision...')

    // Analyze image with GPT-4 Vision
    const analysis = await analyzeImageWithGPT4Vision(imageFile, message)

    console.log('‚úÖ Image analysis completed successfully')

    return NextResponse.json({
      success: true,
      response: analysis,
      type: 'image_analysis'
    })

  } catch (error) {
    console.error('‚ùå Error handling image message:', error)
    return NextResponse.json({ 
      error: 'Kh√¥ng th·ªÉ x·ª≠ l√Ω ·∫£nh. Vui l√≤ng th·ª≠ l·∫°i v·ªõi ·∫£nh kh√°c.',
      details: error instanceof Error ? error.message : 'Unknown error'
    }, { status: 500 })
  }
}

async function analyzeImageWithGPT4Vision(imageFile: File, userMessage: string) {
  try {
    console.log('üîç Converting image to base64...')
    
    // Validate image file
    if (!imageFile.type.startsWith('image/')) {
      throw new Error('File is not an image')
    }
    
    if (imageFile.size > 20 * 1024 * 1024) { // 20MB limit
      throw new Error('Image file is too large (max 20MB)')
    }
    
    // Convert image to base64
    const bytes = await imageFile.arrayBuffer()
    const base64Image = Buffer.from(bytes).toString('base64')
    const mimeType = imageFile.type

    console.log('üîç Sending request to OpenAI GPT-4o...')
    console.log('üîç Image size:', imageFile.size, 'bytes')
    console.log('üîç MIME type:', mimeType)

    const response = await openai.chat.completions.create({
      model: "gpt-4o",
      messages: [
        {
          role: "user",
          content: [
            {
              type: "text",
              text: `B·∫°n l√† chuy√™n gia th·ªùi trang. H√£y ph√¢n t√≠ch trang ph·ª•c trong ·∫£nh v√† ƒë∆∞a ra l·ªùi khuy√™n th·ªùi trang.

Y√äU C·∫¶U: ${userMessage || 'Ph√¢n t√≠ch trang ph·ª•c v√† g·ª£i √Ω c·∫£i thi·ªán'}

NHI·ªÜM V·ª§: Ch·ªâ t·∫≠p trung v√†o trang ph·ª•c, ph·ª• ki·ªán, m√†u s·∫Øc v√† phong c√°ch th·ªùi trang. KH√îNG nh·∫≠n di·ªán ng∆∞·ªùi.

FORMAT TR·∫¢ L·ªúI:
**TRANG PH·ª§C**: [M√¥ t·∫£ m√†u s·∫Øc, ki·ªÉu d√°ng ch√≠nh]
**PHONG C√ÅCH**: [Casual/Formal/Sporty + ho√†n c·∫£nh ph√π h·ª£p]
**ƒêI·ªÇM T√çCH C·ª∞C**: [2-3 ƒëi·ªÉm m·∫°nh]
**C·∫¶N C·∫¢I THI·ªÜN**: [1-2 ƒëi·ªÉm c·ª• th·ªÉ]

**G·ª¢I √ù**:
‚Ä¢ [Ph·ª• ki·ªán + m√†u s·∫Øc]
‚Ä¢ [Item thay th·∫ø]
‚Ä¢ [Tip styling]

**SHOPEE**: [2 s·∫£n ph·∫©m - t√™n + m√†u + l√Ω do]

Tr·∫£ l·ªùi ng·∫Øn g·ªçn, chuy√™n nghi·ªáp. K·∫øt th√∫c b·∫±ng c√¢u h·ªèi.`
            },
            {
              type: "image_url",
              image_url: {
                url: `data:${mimeType};base64,${base64Image}`
              }
            }
          ]
        }
      ],
      max_tokens: 1200,
      temperature: 0.7
    })

    const analysis = response.choices[0].message.content
    console.log('‚úÖ GPT-4o analysis completed successfully')
    console.log('üîç Analysis length:', analysis?.length || 0)
    
    if (!analysis) {
      throw new Error('No analysis returned from OpenAI')
    }
    
    return analysis

  } catch (error) {
    console.error('‚ùå GPT-4 Vision error:', error)
    
    if (error instanceof Error) {
      if (error.message.includes('API key')) {
        return "L·ªói c·∫•u h√¨nh API. Vui l√≤ng li√™n h·ªá qu·∫£n tr·ªã vi√™n."
      }
      if (error.message.includes('quota') || error.message.includes('limit')) {
        return "ƒê√£ v∆∞·ª£t qu√° gi·ªõi h·∫°n s·ª≠ d·ª•ng. Vui l√≤ng th·ª≠ l·∫°i sau."
      }
      if (error.message.includes('image')) {
        return "·∫¢nh kh√¥ng h·ª£p l·ªá ho·∫∑c qu√° l·ªõn. Vui l√≤ng ch·ªçn ·∫£nh kh√°c (t·ªëi ƒëa 20MB)."
      }
      if (error.message.includes("can't help") || error.message.includes("identifying")) {
        return `**TRANG PH·ª§C**: T√¥i th·∫•y b·∫°n ƒë√£ g·ª≠i ·∫£nh trang ph·ª•c
**PHONG C√ÅCH**: ƒê·ªÉ ph√¢n t√≠ch ch√≠nh x√°c, h√£y m√¥ t·∫£ trang ph·ª•c b·∫°n ƒëang m·∫∑c
**ƒêI·ªÇM T√çCH C·ª∞C**: B·∫°n c√≥ th·ªÉ chia s·∫ª th√™m v·ªÅ m√†u s·∫Øc, ki·ªÉu d√°ng
**C·∫¶N C·∫¢I THI·ªÜN**: C·∫ßn th√™m th√¥ng tin ƒë·ªÉ t∆∞ v·∫•n t·ªët h∆°n

**G·ª¢I √ù**:
‚Ä¢ H√£y m√¥ t·∫£ trang ph·ª•c trong ·∫£nh
‚Ä¢ Cho bi·∫øt b·∫°n mu·ªën t∆∞ v·∫•n g√¨
‚Ä¢ G·ª≠i link s·∫£n ph·∫©m Shopee thay th·∫ø

**SHOPEE**: B·∫°n c√≥ th·ªÉ g·ª≠i link s·∫£n ph·∫©m ƒë·ªÉ t√¥i ph√¢n t√≠ch chi ti·∫øt

B·∫°n c√≥ th·ªÉ m√¥ t·∫£ trang ph·ª•c trong ·∫£nh kh√¥ng?`
      }
    }
    
    return "Xin l·ªói, t√¥i kh√¥ng th·ªÉ ph√¢n t√≠ch ·∫£nh n√†y. Vui l√≤ng th·ª≠ l·∫°i v·ªõi ·∫£nh kh√°c ho·∫∑c g·ª≠i link s·∫£n ph·∫©m Shopee ƒë·ªÉ t√¥i c√≥ th·ªÉ t∆∞ v·∫•n cho b·∫°n nh√©! üòä"
  }
}

